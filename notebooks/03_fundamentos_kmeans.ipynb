{"cells":[{"cell_type":"markdown","metadata":{"id":"fSrLSIPMKVJf"},"source":["# Notebook 03: Fundamentos de k-means"]},{"cell_type":"markdown","source":["En este notebook se va a implementar el algoritmo de clustering K-Means. En la primera parte se explica la implementación manual del algoritmo y luego se muestra como realizar los cálculos de forma sencilla con la librería sklearn."],"metadata":{"id":"_6Vdl0NrWm2y"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf # Solamente lo utilizamos para descargar los datos\n","import matplotlib.pyplot as plt\n","from sklearn.metrics.pairwise import euclidean_distances"],"metadata":{"id":"y96tVVqdWaIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d1 = np.random.randn(20, 2) + 2\n","d2 = np.random.randn(20, 2) - 3\n","d3 = np.random.randn(20, 2)\n","d3[:, 0] = d3[:, 0] + 4\n","d3[:, 1] = d3[:, 1] - 4"],"metadata":{"id":"GvQhSnXWWaK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(d1[:, 0], d1[:, 1], '.')\n","plt.plot(d2[:, 0], d2[:, 1], '.')\n","plt.plot(d3[:, 0], d3[:, 1], '.')\n","plt.show()"],"metadata":{"id":"_aeJZ4hGWaNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datos = np.concatenate((d1, d2, d3), axis=0)\n","datos = datos[np.random.permutation(len(datos))]"],"metadata":{"id":"S8X5lxMhWaP9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implementación manual del algoritmo"],"metadata":{"id":"AW9D4bAxWwK0"}},{"cell_type":"code","source":["K = 3"],"metadata":{"id":"ERMlgW8kWskR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inicializar los centroides\n","# Se seleccionan puntos aleatorios del conjunto de datos\n","\n","\n","# While no se cumpla la condicion de parada do\n","# Condicion de parada: Que en una iteración del algoritmo no se modifique ningún centroide\n","\n","    # Asignar cada dato xi al centroide más cercano\n","\n","    # Actualizar los centroides según cierta operación\n","    # Operación: Actualizar el centroide por el promedio de los puntos del cluster\n"],"metadata":{"id":"ZsundEuNWsmV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clusters"],"metadata":{"id":"yzYdu_2cZZSj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["color = [\"red\", \"green\", \"blue\"]\n","for c in np.unique(clusters):\n","  plt.plot(datos[clusters == c, 0], datos[clusters == c, 1], '.', color=color[c], label=\"Cluster \"+str(c+1), alpha=0.3)\n","  plt.scatter([centroides[c, 0]], [centroides[c, 1]], color=color[c], edgecolors=\"black\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"pKfwqvHVZjn7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## K-Means usando la librería Sklearn"],"metadata":{"id":"r8eb381VaRtJ"}},{"cell_type":"markdown","source":["https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"],"metadata":{"id":"UmChxBgjdwsi"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"5J4tUwpQKVJu"},"outputs":[],"source":["from sklearn.cluster import KMeans"]},{"cell_type":"code","source":["K = 3\n","\n","kmeans = KMeans(n_clusters = K, init = 'random', n_init = 10).fit(datos)"],"metadata":{"id":"iJXnglZ1aWEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clusters = kmeans.predict(datos)\n","clusters"],"metadata":{"id":"b_2r_v5qaXv9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["color = [\"red\", \"green\", \"blue\"]\n","for c in np.unique(clusters):\n","  plt.plot(datos[clusters == c, 0], datos[clusters == c, 1], '.', color=color[c], label=\"Cluster \"+str(c+1), alpha=0.3)\n","  plt.scatter([kmeans.cluster_centers_[c, 0]], [kmeans.cluster_centers_[c, 1]], color=color[c], edgecolors=\"black\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Qqq3b8a6e0HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Frontera de clusterización con K-Means"],"metadata":{"id":"xYF_uYt3fHWf"}},{"cell_type":"code","source":["margen = 0.5\n","XX, YY = np.meshgrid(np.linspace(datos.min(axis=0)[0] - margen, datos.max(axis=0)[0] + margen, 100), np.linspace(datos.min(axis=0)[1] - margen, datos.max(axis=0)[1] + margen, 100))\n","points = np.concatenate([XX.reshape(-1, 1), YY.reshape(-1, 1)], axis=1)\n","labels = kmeans.predict(points)\n","labels = labels.reshape(XX.shape)"],"metadata":{"id":"FF0wwrSjfLdQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["color = [\"red\", \"green\", \"blue\"]\n","for c in np.unique(clusters):\n","  plt.plot(datos[clusters == c, 0], datos[clusters == c, 1], '.', color=color[c], label=\"Cluster \"+str(c+1), alpha=0.5)\n","  plt.scatter([kmeans.cluster_centers_[c, 0]], [kmeans.cluster_centers_[c, 1]], color=color[c], edgecolors=\"black\")\n","  plt.scatter(XX[labels == c], YY[labels == c], color=color[c], marker='.', alpha=0.08)\n","plt.show()"],"metadata":{"id":"iop59y9kfjt2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# K-Means para MNIST"],"metadata":{"id":"pAG2RFAPbgdu"}},{"cell_type":"markdown","source":["Vamos a descargar el dataset de MNIST para hacer clustering con los datos de MNIST:"],"metadata":{"id":"GbdAZ4JXbncm"}},{"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"],"metadata":{"id":"Kaz-EYykajp_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dividimos entre 255 para que todos los atributos de la imagen estén entre 0 y 1."],"metadata":{"id":"lvkzg1CSbp9d"}},{"cell_type":"code","source":["x_train = x_train / 255\n","x_test = x_test / 255"],"metadata":{"id":"flqiqiSoborf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Restamos ahora la media para destacar como positivos los píxeles de la clase y el resto con valores negativos."],"metadata":{"id":"WwcEYHdAbrwu"}},{"cell_type":"code","source":["x_train = x_train - x_train.mean(axis=0)\n","x_test = x_test - x_train.mean(axis=0)"],"metadata":{"id":"WGt0ASFPbrG6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Veamos el aspecto de nuestros datos una vez más:"],"metadata":{"id":"TGnZy_6Zbx3D"}},{"cell_type":"code","source":["plt.figure(figsize=(15,4))\n","for i in range(20):\n","  plt.subplot(2,10,i+1)\n","  plt.imshow(x_train[np.random.randint(60000)], cmap=\"bwr\", vmin=-1, vmax=1)\n","plt.show()"],"metadata":{"id":"jN-58h_qbwvM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datos = x_train.reshape(x_train.shape[0], -1)\n","datos.shape"],"metadata":{"id":"jwtJutlPbzmu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para agilizar los cálculos (60000 imágenes tarda un buen rato) vamos a reducir el número de imágenes a 10000."],"metadata":{"id":"12gzaiuxcLXV"}},{"cell_type":"code","source":["datos = datos[:10000]\n","datos.shape"],"metadata":{"id":"9WdMDN_FcNvM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Empezamos con el algoritmo. Recuerda definir $K$."],"metadata":{"id":"RmaVCr49b3YF"}},{"cell_type":"markdown","source":["- Probamos con K = 10, el número de clases"],"metadata":{"id":"0ZBzM3_OcCNw"}},{"cell_type":"code","source":["K = 10\n","\n","kmeans = KMeans(n_clusters = K, init = 'random', n_init = 10).fit(datos)\n","clusters = kmeans.predict(datos)\n","clusters"],"metadata":{"id":"m48njXocb1ST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(clusters, bins=K)\n","plt.show()"],"metadata":{"id":"RwH3xBw5cGqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for c in np.unique(clusters):\n","  indices = np.where(clusters == c)[0]\n","  plt.figure(figsize=(15,4))\n","  plt.title(\"Cluster \" + str(c))\n","  for i in range(10):\n","    plt.subplot(1,10,i+1)\n","    plt.imshow(x_train[indices][i], cmap=\"bwr\", vmin=-1, vmax=1)\n","  plt.show()"],"metadata":{"id":"MYbfB1NZcgpl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for center in kmeans.cluster_centers_:\n","  plt.figure(figsize=(2,2))\n","  plt.imshow(center.reshape(28,28), cmap=\"bwr\", vmin=-1, vmax=1)\n","  plt.show()"],"metadata":{"id":"ENPavXyjivI7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Discusión de los resultados**\n","\n","No tiene sentido seleccionar un $K$ igual al número de clases. Estamos en clustering, no en aprendizaje supervisado!"],"metadata":{"id":"KWMep3Sfc-EQ"}},{"cell_type":"markdown","source":["**Ejercicio**: Busca un K razonable y cuéntanos qué conclusiones sacas."],"metadata":{"id":"7Nnuame4dJC-"}},{"cell_type":"code","source":["K = 30\n","\n","kmeans = KMeans(n_clusters = K, init = 'random', n_init = 10).fit(datos)\n","clusters = kmeans.predict(datos)\n","clusters"],"metadata":{"id":"uwcEv9hpczTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(clusters, bins=K)\n","plt.show()"],"metadata":{"id":"PQyKXfbIdRHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for c in np.unique(clusters):\n","  indices = np.where(clusters == c)[0]\n","  if len(indices) > 10:\n","    plt.figure(figsize=(15,4))\n","    plt.title(\"Cluster \" + str(c))\n","    for i in range(10):\n","      plt.subplot(1,10,i+1)\n","      plt.imshow(x_train[indices][i], cmap=\"bwr\", vmin=-1, vmax=1)\n","    plt.show()"],"metadata":{"id":"T_xTdsI6dRhO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for center in kmeans.cluster_centers_:\n","  plt.figure(figsize=(2,2))\n","  plt.imshow(center.reshape(28,28), cmap=\"bwr\", vmin=-1, vmax=1)\n","  plt.show()"],"metadata":{"id":"Sy2M5UVadbyn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# K-Means para Breast Cancer"],"metadata":{"id":"FutGv56-jiFT"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","import pandas as pd"],"metadata":{"id":"nDzEHYsSd0qp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["breastCancer = load_breast_cancer()\n","print(breastCancer.DESCR)"],"metadata":{"id":"1KPrJ115juXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datos = pd.DataFrame(breastCancer.data, columns=breastCancer.feature_names)\n","datos"],"metadata":{"id":"ObkQqV60jxQw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocesado"],"metadata":{"id":"juVYY6tCkSJD"}},{"cell_type":"code","source":["medias = datos.mean()\n","stds = datos.std()\n","datos = (datos - medias) / stds\n","datos"],"metadata":{"id":"toKwwzkIkGAK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejecución"],"metadata":{"id":"MBxPs07SknNT"}},{"cell_type":"code","source":["K = 10\n","\n","kmeans = KMeans(n_clusters = K, init = 'random', n_init = 10).fit(datos)\n","clusters = kmeans.predict(datos)\n","clusters"],"metadata":{"id":"HDyIzwhakdPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["centroides = pd.DataFrame(kmeans.cluster_centers_, columns=datos.columns)\n","centroides"],"metadata":{"id":"9lpUj4dzkvVD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reducción de dimensionalidad para visualización"],"metadata":{"id":"M-_78F-DlPIQ"}},{"cell_type":"markdown","source":["Recordad que PCA es una herramienta muy útil para reducir la dimensionalidad de vuestro problema. Además, se puede utilizar para intentar **proyectar un espacio N-dimensional a 2 dimensiones**. Vamos a utilizarlo."],"metadata":{"id":"eTAfabWYmktL"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA"],"metadata":{"id":"14HZn1aZk7U5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aunque nos va a dar igual, porque nuestro objetivo es proyectar a 2 dimensiones, conviene analizar la varianza explicada. Para ello calculamos un primer PCA con el número de componentes igual al número de atributos."],"metadata":{"id":"nYzp2Xd3m1fE"}},{"cell_type":"code","source":["pca = PCA(n_components=datos.shape[1])\n","pca.fit(datos)"],"metadata":{"id":"TLXHPm9xmx_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como se ve en la siguiente figura, PCA tiene una varianza explicada de aproximadamente un 65% con 2 componentes. **No es un buen resultado** pero queremos visualizar nuestros datos."],"metadata":{"id":"tjprt-xPmxJE"}},{"cell_type":"code","source":["plt.plot(pca.explained_variance_ratio_.cumsum())\n","plt.grid()\n","plt.show()"],"metadata":{"id":"tIojN7QYlSGE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora sí, calculamos PCA para 2 componentes y lo pintamos."],"metadata":{"id":"8q9sTz-5nbhF"}},{"cell_type":"code","source":["pca = PCA(n_components=2)\n","pca.fit(datos)\n","datos_2d = pca.transform(datos)"],"metadata":{"id":"vZ2dA2nQlY-s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Esta primera figura **No nos da mucha información**, no se ve a priori ningún grupo diferenciado en el dataset."],"metadata":{"id":"sAmoY8uEngCR"}},{"cell_type":"code","source":["plt.plot(datos_2d[:, 0], datos_2d[:, 1], '.', alpha=0.3)\n","plt.show()"],"metadata":{"id":"chu-FMCTlxrO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sin embargo, si incorporamos la información que nos proporciona K-Means, es decir, a qué cluster pertenece cada punto, podemos visualizar un poco mejor los datos.\n","\n","- Hay grupos bien diferenciados (se ven claramente grupos de distinto color).\n","\n","- Hay grupos que PCA no ha sido capaz de diferenciar, los agrupa todos en el mismo sitio pero K-Means es capaz de identificarlos.\n","\n","- Hay grupos que en PCA están dispersados pero que K-Means los está reconociendo como cercanos."],"metadata":{"id":"Yo9jSdQEnns_"}},{"cell_type":"code","source":["for c in np.unique(clusters):\n","  plt.plot(datos_2d[clusters==c, 0], datos_2d[clusters==c, 1], '.')\n","plt.show()"],"metadata":{"id":"2aOYsQihlz-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conclusión"],"metadata":{"id":"GmfegdZ-oH4o"}},{"cell_type":"markdown","source":["- Hemos comprendido el funcionamiento de K-Means.\n","\n","- Hemos implementado K-Means con selección aleatoria a mano.\n","\n","- Hemos recordado el concepto de que al no estar en un problema supervisado, ajustar la K al número de clases que esperamos no tiene mucho sentido.\n","\n","- Hemos comprobado la importancia de seleccionar una buena K.\n","\n","- Hemos aprendido a usar PCA con clustering para visualizar en 2 dimensiones datos de alta dimensionalidad."],"metadata":{"id":"RdyMjR0XoLeP"}},{"cell_type":"code","source":[],"metadata":{"id":"EyP2S0I1l6aL"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}